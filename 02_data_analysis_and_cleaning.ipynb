{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.impute import KNNImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load pandas dataframes from pkl files\n",
    "all_depression_train = pd.read_pickle(\"data/train.pkl\")\n",
    "all_depression_test = pd.read_pickle(\"data/test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# columns with polygenetic risk scores (PRS) from UK Biobank\n",
    "PRS_columns = [\n",
    "    \"p26202\", \"p26203\", \"p26204\", \"p26205\", \"p26206\", \"p26207\", \"p26208\", \"p26209\", \n",
    "    \"p26210\", \"p26211\", \"p26212\", \"p26213\", \"p26214\", \"p26215\", \"p26216\", \"p26217\", \n",
    "    \"p26218\", \"p26219\", \"p26220\", \"p26221\", \"p26222\", \"p26223\", \"p26224\", \"p26225\", \n",
    "    \"p26226\", \"p26227\", \"p26228\", \"p26229\", \"p26231\", \"p26232\", \"p26233\", \"p26234\", \n",
    "    \"p26235\", \"p26236\", \"p26237\", \"p26238\", \"p26239\", \"p26240\", \"p26241\", \"p26242\", \n",
    "    \"p26243\", \"p26244\", \"p26245\", \"p26246\", \"p26247\", \"p26248\", \"p26249\", \"p26250\", \n",
    "    \"p26251\", \"p26252\", \"p26253\", \"p26254\", \"p26255\", \"p26256\", \"p26257\", \"p26258\", \n",
    "    \"p26259\", \"p26260\", \"p26261\", \"p26262\", \"p26263\", \"p26264\", \"p26265\", \"p26266\", \n",
    "    \"p26267\", \"p26268\", \"p26269\", \"p26270\", \"p26271\", \"p26272\", \"p26273\", \"p26274\", \n",
    "    \"p26275\", \"p26276\", \"p26277\", \"p26278\", \"p26279\", \"p26280\", \"p26281\", \"p26282\", \n",
    "    \"p26283\", \"p26284\", \"p26285\", \"p26286\", \"p26287\", \"p26289\", \"p26290\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: p26202\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26203\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26204\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26205\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26206\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26207\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26208\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26209\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26210\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26211\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26212\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26213\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26214\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26215\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26216\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26217\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26218\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26219\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26220\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26221\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26222\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26223\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26224\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26225\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26226\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26227\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26228\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26229\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26231\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26232\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26233\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26234\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26235\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26236\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26237\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26238\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26239\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26240\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26241\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26242\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26243\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26244\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26245\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26246\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26247\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26248\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26249\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26250\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26251\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26252\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26253\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26254\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26255\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26256\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26257\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26258\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26259\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26260\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26261\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26262\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26263\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26264\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26265\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26266\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26267\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26268\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26269\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26270\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26271\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26272\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26273\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26274\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26275\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26276\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26277\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26278\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26279\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26280\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26281\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26282\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26283\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26284\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26285\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26286\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26287\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26289\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n",
      "Column: p26290\n",
      "Missing values: 0\n",
      "Percentage of missing values: 0.00%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check missing values and percentages for all columns in PRS_columns\n",
    "def check_missing_values(df, columns):\n",
    "    # Total number of rows in the DataFrame\n",
    "    total_rows = df.shape[0]\n",
    "    \n",
    "    # Iterate over each column in the PRS_columns list\n",
    "    for column in columns:\n",
    "        if column in df.columns:\n",
    "            # Check the number of missing values\n",
    "            missing_values = df[column].isnull().sum()\n",
    "            \n",
    "            # Calculate the percentage of missing values\n",
    "            missing_percentage = (missing_values / total_rows) * 100\n",
    "            \n",
    "            # Print the results for the column\n",
    "            print(f\"Column: {column}\")\n",
    "            print(f\"Missing values: {missing_values}\")\n",
    "            print(f\"Percentage of missing values: {missing_percentage:.2f}%\\n\")\n",
    "        else:\n",
    "            print(f\"Column: {column} is not found in the DataFrame.\\n\")\n",
    "\n",
    "# Apply the function to the all_depression_train dataset\n",
    "check_missing_values(all_depression_train, PRS_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# define size of each dataset\n",
    "all_depression_train_size = 500\n",
    "all_depression_test_size = 500\n",
    "\n",
    "# randomly sample to the desired dataset sizes\n",
    "# Downsample datasets\n",
    "all_depression_train = all_depression_train.sample(all_depression_train_size, random_state=81)\n",
    "all_depression_test = all_depression_test.sample(all_depression_test_size, random_state=81)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_depression_train - Number of rows: 500\n",
      "0    50.2\n",
      "1    49.8\n",
      "Name: target, dtype: float64\n",
      "\n",
      "\n",
      "all_depression_test - Number of rows: 500\n",
      "0    51.2\n",
      "1    48.8\n",
      "Name: target, dtype: float64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the number of rows and balance for each dataset\n",
    "def print_dataset_info(dataset, dataset_name):\n",
    "    print(f\"{dataset_name} - Number of rows: {len(dataset)}\")\n",
    "    print(dataset['target'].value_counts(normalize=True) * 100)\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Print information for each dataset\n",
    "print_dataset_info(all_depression_train, \"all_depression_train\")\n",
    "print_dataset_info(all_depression_test, \"all_depression_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## descriptives of the two datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            sex  age_at_baseline  depression source_of_depression_reporting  \\\n",
      "126  Category_A               68           1                     Category_E   \n",
      "314  Category_A               55           0                     Category_E   \n",
      "267  Category_A               63           1                     Category_E   \n",
      "282  Category_A               56           0                     Category_B   \n",
      "496  Category_E               55           0                     Category_C   \n",
      "\n",
      "     months_depression_after_baseline  \n",
      "126                         95.933333  \n",
      "314                         51.100000  \n",
      "267                        -30.833333  \n",
      "282                        143.966667  \n",
      "496                         58.600000  \n",
      "            sex  age_at_baseline  depression source_of_depression_reporting  \\\n",
      "126  Category_E               51           0                     Category_C   \n",
      "314  Category_D               49           0                     Category_D   \n",
      "267  Category_C               68           0                     Category_D   \n",
      "282  Category_D               56           1                     Category_A   \n",
      "496  Category_A               67           1                     Category_C   \n",
      "\n",
      "     months_depression_after_baseline  \n",
      "126                         93.966667  \n",
      "314                        155.733333  \n",
      "267                        184.766667  \n",
      "282                        102.166667  \n",
      "496                         -3.866667  \n"
     ]
    }
   ],
   "source": [
    "# filter the following columns from train and test dataset\n",
    "# p31 - sex\n",
    "# age_at_baseline\n",
    "# target - rename to depression\n",
    "# p130895 - rename to source_of_depression_reporting\n",
    "# new column - p53_i0 minus p130894 - name months_depression_after_baseline\n",
    "\n",
    "# For training data\n",
    "train_descriptives = all_depression_train[['p31', 'age_at_baseline', 'target', 'p130895', 'p53_i0', 'p130894']].copy()\n",
    "train_descriptives.rename(columns={\n",
    "    'p31': 'sex',\n",
    "    'target': 'depression',\n",
    "    'p130895': 'source_of_depression_reporting'\n",
    "}, inplace=True)\n",
    "\n",
    "# Ensure the columns are in datetime format\n",
    "train_descriptives['p53_i0'] = pd.to_datetime(train_descriptives['p53_i0'])\n",
    "train_descriptives['p130894'] = pd.to_datetime(train_descriptives['p130894'])\n",
    "\n",
    "# Calculate the difference in days, convert to months, and convert to int\n",
    "train_descriptives['months_depression_after_baseline'] = (train_descriptives['p130894'] - train_descriptives['p53_i0']).dt.days / 30\n",
    "\n",
    "# Drop the unnecessary columns after calculation\n",
    "train_descriptives.drop(columns=['p53_i0', 'p130894'], inplace=True)\n",
    "\n",
    "# For testing data\n",
    "test_descriptives = all_depression_test[['p31', 'age_at_baseline', 'target', 'p130895', 'p53_i0', 'p130894']].copy()\n",
    "test_descriptives.rename(columns={\n",
    "    'p31': 'sex',\n",
    "    'target': 'depression',\n",
    "    'p130895': 'source_of_depression_reporting'\n",
    "}, inplace=True)\n",
    "\n",
    "# Ensure the columns are in datetime format\n",
    "test_descriptives['p53_i0'] = pd.to_datetime(test_descriptives['p53_i0'])\n",
    "test_descriptives['p130894'] = pd.to_datetime(test_descriptives['p130894'])\n",
    "\n",
    "# Calculate the difference in days, convert to months, and convert to int\n",
    "test_descriptives['months_depression_after_baseline'] = (test_descriptives['p130894'] - test_descriptives['p53_i0']).dt.days / 30\n",
    "\n",
    "# Drop the unnecessary columns after calculation\n",
    "test_descriptives.drop(columns=['p53_i0', 'p130894'], inplace=True)\n",
    "\n",
    "# Print out the first few rows for both dataframes\n",
    "print(train_descriptives.head())\n",
    "print(test_descriptives.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data - Min months: -49.333333333333336, Max months: 195.6\n",
      "Test data - Min months: -50.46666666666667, Max months: 199.0\n"
     ]
    }
   ],
   "source": [
    "# Filter rows where depression == 1 for train data\n",
    "train_depression_rows = train_descriptives[train_descriptives['depression'] == 1]\n",
    "\n",
    "# Calculate min and max for months_depression_after_baseline in train data\n",
    "train_min_months = train_depression_rows['months_depression_after_baseline'].min()\n",
    "train_max_months = train_depression_rows['months_depression_after_baseline'].max()\n",
    "\n",
    "print(f\"Train data - Min months: {train_min_months}, Max months: {train_max_months}\")\n",
    "\n",
    "# Filter rows where depression == 1 for test data\n",
    "test_depression_rows = test_descriptives[test_descriptives['depression'] == 1]\n",
    "\n",
    "# Calculate min and max for months_depression_after_baseline in test data\n",
    "test_min_months = test_depression_rows['months_depression_after_baseline'].min()\n",
    "test_max_months = test_depression_rows['months_depression_after_baseline'].max()\n",
    "\n",
    "print(f\"Test data - Min months: {test_min_months}, Max months: {test_max_months}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated unique values in source_of_depression_reporting (train):\n",
      "['Category_E' 'Category_B' 'Category_C' 'Category_D' 'Category_A']\n",
      "Updated unique values in source_of_depression_reporting (test):\n",
      "['Category_C' 'Category_D' 'Category_A' 'Category_B' 'Category_E']\n"
     ]
    }
   ],
   "source": [
    "# Define the mapping for values in source_of_depression_reporting\n",
    "replace_mapping = {\n",
    "    'Self-report only': 'self-report',\n",
    "    'Self-report and other source(s)': 'self-report',\n",
    "    'Primary care only': 'primary care',\n",
    "    'Primary care and other source(s)': 'primary care',\n",
    "    'Hospital admissions data only': 'hospital admission',\n",
    "    'Hospital admissions data and other source(s)': 'hospital admission',\n",
    "    'Death register only': 'hospital admission'\n",
    "}\n",
    "\n",
    "# Apply the mapping to train data\n",
    "train_descriptives['source_of_depression_reporting'] = train_descriptives['source_of_depression_reporting'].replace(replace_mapping)\n",
    "\n",
    "# Apply the mapping to test data\n",
    "test_descriptives['source_of_depression_reporting'] = test_descriptives['source_of_depression_reporting'].replace(replace_mapping)\n",
    "\n",
    "# Print the updated unique values to verify the changes\n",
    "print(\"Updated unique values in source_of_depression_reporting (train):\")\n",
    "print(train_descriptives['source_of_depression_reporting'].unique())\n",
    "\n",
    "print(\"Updated unique values in source_of_depression_reporting (test):\")\n",
    "print(test_descriptives['source_of_depression_reporting'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Function to calculate the descriptive statistics\n",
    "def create_summary(df):\n",
    "    summary = pd.DataFrame()\n",
    "\n",
    "    # Sex (empty row)\n",
    "    summary.loc['sex', 'percentage'] = ''\n",
    "    summary.loc['Female', 'percentage'] = df['sex'].value_counts(normalize=True).get('Female', 0) * 100\n",
    "    summary.loc['Male', 'percentage'] = df['sex'].value_counts(normalize=True).get('Male', 0) * 100\n",
    "\n",
    "    # Age at baseline (mean and standard deviation)\n",
    "    summary.loc['age_at_baseline', 'mean'] = df['age_at_baseline'].mean()\n",
    "    summary.loc['age_at_baseline', 'standard deviation'] = df['age_at_baseline'].std()\n",
    "\n",
    "    # Depression (empty row)\n",
    "    summary.loc['depression', 'percentage'] = ''\n",
    "    summary.loc['Yes', 'percentage'] = df['depression'].value_counts(normalize=True).get(1, 0) * 100\n",
    "    summary.loc['No', 'percentage'] = df['depression'].value_counts(normalize=True).get(0, 0) * 100\n",
    "\n",
    "    # Months depression after baseline (mean and standard deviation, only for rows with depression == 1)\n",
    "    depression_rows = df[df['depression'] == 1]\n",
    "    summary.loc['months_depression_after_baseline', 'mean'] = depression_rows['months_depression_after_baseline'].mean()\n",
    "    summary.loc['months_depression_after_baseline', 'standard deviation'] = depression_rows['months_depression_after_baseline'].std()\n",
    "\n",
    "    # Source of reporting (empty row)\n",
    "    summary.loc['source_of_depression_reporting', 'percentage'] = ''\n",
    "    # Categories of source_of_depression_reporting with percentages\n",
    "    for category in df['source_of_depression_reporting'].unique():\n",
    "        if category is not None:\n",
    "            summary.loc[category, 'percentage'] = df['source_of_depression_reporting'].value_counts(normalize=True).get(category, 0) * 100\n",
    "\n",
    "    return summary\n",
    "\n",
    "# Calculate the descriptive statistics for train and test data\n",
    "train_summary = create_summary(train_descriptives)\n",
    "test_summary = create_summary(test_descriptives)\n",
    "\n",
    "# Save the descriptives to CSV\n",
    "train_summary.to_csv('data/train_descriptives_summary.csv')\n",
    "test_summary.to_csv('data/test_descriptives_summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>percentage</th>\n",
       "      <th>mean</th>\n",
       "      <th>standard deviation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sex</th>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Female</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Male</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age_at_baseline</th>\n",
       "      <td>NaN</td>\n",
       "      <td>54.778000</td>\n",
       "      <td>8.901190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>depression</th>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Yes</th>\n",
       "      <td>49.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No</th>\n",
       "      <td>50.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>months_depression_after_baseline</th>\n",
       "      <td>NaN</td>\n",
       "      <td>79.195181</td>\n",
       "      <td>61.994047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source_of_depression_reporting</th>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Category_E</th>\n",
       "      <td>20.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Category_B</th>\n",
       "      <td>19.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Category_C</th>\n",
       "      <td>18.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Category_D</th>\n",
       "      <td>20.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Category_A</th>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 percentage       mean  standard deviation\n",
       "sex                                                NaN                 NaN\n",
       "Female                                    0        NaN                 NaN\n",
       "Male                                      0        NaN                 NaN\n",
       "age_at_baseline                         NaN  54.778000            8.901190\n",
       "depression                                         NaN                 NaN\n",
       "Yes                                    49.8        NaN                 NaN\n",
       "No                                     50.2        NaN                 NaN\n",
       "months_depression_after_baseline        NaN  79.195181           61.994047\n",
       "source_of_depression_reporting                     NaN                 NaN\n",
       "Category_E                             20.8        NaN                 NaN\n",
       "Category_B                             19.8        NaN                 NaN\n",
       "Category_C                             18.8        NaN                 NaN\n",
       "Category_D                             20.6        NaN                 NaN\n",
       "Category_A                             20.0        NaN                 NaN"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_summary.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>percentage</th>\n",
       "      <th>mean</th>\n",
       "      <th>standard deviation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sex</th>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Female</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Male</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age_at_baseline</th>\n",
       "      <td>NaN</td>\n",
       "      <td>55.016000</td>\n",
       "      <td>8.785791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>depression</th>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Yes</th>\n",
       "      <td>48.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No</th>\n",
       "      <td>51.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>months_depression_after_baseline</th>\n",
       "      <td>NaN</td>\n",
       "      <td>70.767623</td>\n",
       "      <td>61.642417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source_of_depression_reporting</th>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Category_C</th>\n",
       "      <td>20.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Category_D</th>\n",
       "      <td>20.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Category_A</th>\n",
       "      <td>18.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Category_B</th>\n",
       "      <td>18.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Category_E</th>\n",
       "      <td>21.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 percentage       mean  standard deviation\n",
       "sex                                                NaN                 NaN\n",
       "Female                                    0        NaN                 NaN\n",
       "Male                                      0        NaN                 NaN\n",
       "age_at_baseline                         NaN  55.016000            8.785791\n",
       "depression                                         NaN                 NaN\n",
       "Yes                                    48.8        NaN                 NaN\n",
       "No                                     51.2        NaN                 NaN\n",
       "months_depression_after_baseline        NaN  70.767623           61.642417\n",
       "source_of_depression_reporting                     NaN                 NaN\n",
       "Category_C                             20.6        NaN                 NaN\n",
       "Category_D                             20.6        NaN                 NaN\n",
       "Category_A                             18.8        NaN                 NaN\n",
       "Category_B                             18.6        NaN                 NaN\n",
       "Category_E                             21.4        NaN                 NaN"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_summary.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define special columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# special columns which will be removed from train datasets and ignored for all cleaning in test datasets\n",
    "special_columns = ['eid', 'p130894', 'p130895', 'p53_i0']\n",
    "# eid (participants ID)\n",
    "# p130894 (date of first occurence of depression),\n",
    "# p130895 (source of first occurence of depression)\n",
    "# p53_i0 (date of baseline assessment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# remove special columns from train dataset\n",
    "all_depression_train = all_depression_train.drop(columns=special_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# move special columns to the end\n",
    "# Rearranging columns in all_depression_test\n",
    "cols = [col for col in all_depression_test.columns if col not in special_columns]\n",
    "all_depression_test = all_depression_test[cols + special_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Function to check if a column contains list-like structures\n",
    "def contains_list_structure(column):\n",
    "    return column.apply(lambda x: isinstance(x, list)).any()\n",
    "\n",
    "# Function to identify columns with list structures in a dataframe, excluding special_columns\n",
    "def identify_columns_with_lists(df, special_columns):\n",
    "    return df.columns[~df.columns.isin(special_columns)][df.loc[:, ~df.columns.isin(special_columns)].apply(contains_list_structure)]\n",
    "\n",
    "# Identify columns with list structures in all datasets, excluding special_columns\n",
    "columns_with_lists_all_train = identify_columns_with_lists(all_depression_train, special_columns)\n",
    "columns_with_lists_all_test = identify_columns_with_lists(all_depression_test, special_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Function to convert list-like structures to pure strings\n",
    "def convert_to_string(value):\n",
    "    if isinstance(value, list):\n",
    "        return ', '.join(map(str, value))  # Join list elements into a single string\n",
    "    elif pd.isna(value):\n",
    "        return ''  # Handle NaN values by converting them to empty strings\n",
    "    else:\n",
    "        return str(value)  # Convert any other type to a string\n",
    "\n",
    "# Function to apply conversion to all object-type columns in a DataFrame, excluding special_columns\n",
    "def convert_columns_to_string(df, special_columns):\n",
    "    for column in df.select_dtypes(include=['object']).columns:\n",
    "        if column not in special_columns:\n",
    "            df[column] = df[column].apply(convert_to_string)\n",
    "    return df\n",
    "\n",
    "# Apply the conversion function to all datasets, excluding special_columns\n",
    "all_depression_train = convert_columns_to_string(all_depression_train, special_columns)\n",
    "all_depression_test = convert_columns_to_string(all_depression_test, special_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Function to convert numeric columns to float64, excluding special_columns and the 'target' column\n",
    "def convert_numeric_to_float64(df, special_columns, target_column='target'):\n",
    "    numeric_columns = df.select_dtypes(include=['number']).columns\n",
    "    columns_to_exclude = special_columns + [target_column]\n",
    "    columns_to_convert = numeric_columns.difference(columns_to_exclude)\n",
    "    df[columns_to_convert] = df[columns_to_convert].astype('float64')\n",
    "    return df\n",
    "\n",
    "# Apply the conversion function to all datasets, excluding special_columns and the target column\n",
    "all_depression_train = convert_numeric_to_float64(all_depression_train, special_columns)\n",
    "all_depression_test = convert_numeric_to_float64(all_depression_test, special_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## handle columns with many missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns with missing values: 0\n",
      "Mean of missing values per column (in %): 0.00%\n",
      "Standard deviation of missing values per column (in %): 0.00%\n",
      "Number of columns with more than 10.0% missing values: 0\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate and print missing value statistics in percentage\n",
    "def calculate_missing_values_statistics(df, missing_percentage=0.10):\n",
    "    # Total number of rows in the dataframe\n",
    "    total_rows = len(df)\n",
    "    \n",
    "    # 1. Number of columns with missing values\n",
    "    columns_with_missing_values = df.columns[df.isnull().sum() > 0]\n",
    "    num_columns_with_missing_values = len(columns_with_missing_values)\n",
    "    print(f\"Number of columns with missing values: {num_columns_with_missing_values}\")\n",
    "    \n",
    "    # 2. Mean and standard deviation of missing values per column (in percentage)\n",
    "    missing_values_per_column = df.isnull().sum() / total_rows * 100  # Convert to percentage\n",
    "    mean_missing_values = missing_values_per_column.mean()\n",
    "    std_missing_values = missing_values_per_column.std()\n",
    "    print(f\"Mean of missing values per column (in %): {mean_missing_values:.2f}%\")\n",
    "    print(f\"Standard deviation of missing values per column (in %): {std_missing_values:.2f}%\")\n",
    "    \n",
    "    # 3. Number of columns with more than a given percentage (e.g., 10%) of missing values\n",
    "    threshold = total_rows * missing_percentage\n",
    "    columns_with_high_missing = df.columns[df.isnull().sum() > threshold]\n",
    "    num_columns_with_high_missing = len(columns_with_high_missing)\n",
    "    print(f\"Number of columns with more than {missing_percentage*100}% missing values: {num_columns_with_high_missing}\")\n",
    "    \n",
    "    return columns_with_missing_values, columns_with_high_missing\n",
    "\n",
    "# Example usage of the function\n",
    "columns_with_missing, columns_with_high_missing = calculate_missing_values_statistics(all_depression_train, missing_percentage=0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Function to drop columns with more than a certain percentage of missing values,\n",
    "# excluding special columns, based only on the training datasets\n",
    "def drop_columns_with_missing_values_based_on_train(train_df, test_df, special_columns, missing_percentage=0.10):\n",
    "    # Calculate the threshold for dropping columns based on the training dataset\n",
    "    threshold = len(train_df) * missing_percentage\n",
    "    \n",
    "    # Get the columns to drop based on the training data\n",
    "    columns_to_drop = train_df.columns[train_df.isnull().sum() > threshold].tolist()\n",
    "    \n",
    "    # Ensure that special columns are not dropped\n",
    "    columns_to_drop = [col for col in columns_to_drop if col not in special_columns]\n",
    "    \n",
    "    # Drop the columns from both training and testing datasets\n",
    "    train_cleaned = train_df.drop(columns=columns_to_drop)\n",
    "    test_cleaned = test_df.drop(columns=columns_to_drop)\n",
    "    \n",
    "    # Ensure special columns are retained\n",
    "    special_columns_to_keep = [col for col in special_columns if col in train_df.columns]\n",
    "    \n",
    "    # Ensure special columns are at the end\n",
    "    train_cleaned = pd.concat([train_cleaned, train_df[special_columns_to_keep]], axis=1)\n",
    "    test_cleaned = pd.concat([test_cleaned, test_df[special_columns_to_keep]], axis=1)\n",
    "    \n",
    "    # Remove any duplicate columns in case they were re-added\n",
    "    train_cleaned = train_cleaned.loc[:, ~train_cleaned.columns.duplicated()]\n",
    "    test_cleaned = test_cleaned.loc[:, ~test_cleaned.columns.duplicated()]\n",
    "    \n",
    "    return train_cleaned, test_cleaned\n",
    "\n",
    "# Apply the function to the datasets\n",
    "all_depression_train_cleaned, all_depression_test_cleaned = drop_columns_with_missing_values_based_on_train(\n",
    "    all_depression_train, all_depression_test, special_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## impute numeric missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of numeric columns with missing values: 0\n",
      "Mean of missing values per numeric column (in %): 0.00%\n",
      "Standard deviation of missing values per numeric column (in %): 0.00%\n",
      "Number of numeric columns with missing values: 0\n",
      "Mean of missing values per numeric column (in %): 0.00%\n",
      "Standard deviation of missing values per numeric column (in %): 0.00%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index([], dtype='object')"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to calculate missing value statistics for numeric columns only\n",
    "def calculate_missing_values_numeric_columns(df, special_columns=None):\n",
    "    if special_columns is None:\n",
    "        special_columns = []\n",
    "    \n",
    "    # Select only the numeric columns for calculation, excluding special columns\n",
    "    numeric_columns = df.select_dtypes(include=['number']).columns.difference(special_columns)\n",
    "    \n",
    "    # Total number of rows in the dataframe\n",
    "    total_rows = len(df)\n",
    "    \n",
    "    # 1. Number of numeric columns with missing values\n",
    "    numeric_columns_with_missing_values = numeric_columns[df[numeric_columns].isnull().sum() > 0]\n",
    "    num_numeric_columns_with_missing_values = len(numeric_columns_with_missing_values)\n",
    "    print(f\"Number of numeric columns with missing values: {num_numeric_columns_with_missing_values}\")\n",
    "    \n",
    "    # 2. Mean and standard deviation of missing values per numeric column (in percentage)\n",
    "    missing_values_per_column_percent = df[numeric_columns].isnull().sum() / total_rows * 100  # Convert to percentage\n",
    "    mean_missing_values_percent = missing_values_per_column_percent.mean()\n",
    "    std_missing_values_percent = missing_values_per_column_percent.std()\n",
    "    print(f\"Mean of missing values per numeric column (in %): {mean_missing_values_percent:.2f}%\")\n",
    "    print(f\"Standard deviation of missing values per numeric column (in %): {std_missing_values_percent:.2f}%\")\n",
    "    \n",
    "    return numeric_columns_with_missing_values\n",
    "\n",
    "# calculate statistics for both datasets\n",
    "calculate_missing_values_numeric_columns(all_depression_train_cleaned, special_columns)\n",
    "calculate_missing_values_numeric_columns(all_depression_test_cleaned, special_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Function to impute missing numeric values with the mean\n",
    "def impute_missing_values_mean(df, special_columns=None):\n",
    "    if special_columns is None:\n",
    "        special_columns = []\n",
    "    \n",
    "    # Select only the numeric columns for imputation, excluding special columns\n",
    "    numeric_columns = df.select_dtypes(include=['number']).columns.difference(special_columns)\n",
    "    \n",
    "    # Impute missing values with the mean for each numeric column\n",
    "    df[numeric_columns] = df[numeric_columns].fillna(df[numeric_columns].mean())\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply the imputation function to both train and test datasets\n",
    "all_depression_train_imputed = impute_missing_values_mean(all_depression_train_cleaned, special_columns)\n",
    "all_depression_test_imputed = impute_missing_values_mean(all_depression_test_cleaned, special_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## impute non-numeric missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed duplicate columns from all_depression_train_imputed.\n",
      "Removed duplicate columns from all_depression_test_imputed.\n"
     ]
    }
   ],
   "source": [
    "# Function to remove duplicate columns in a DataFrame // special columns are accidentally by KNN imputer code duplicated\n",
    "def remove_duplicate_columns(df, df_name):\n",
    "    # Find and remove duplicated columns\n",
    "    df_cleaned = df.loc[:, ~df.columns.duplicated()]\n",
    "    print(f\"Removed duplicate columns from {df_name}.\")\n",
    "    return df_cleaned\n",
    "\n",
    "# Remove duplicate columns from each DataFrame\n",
    "all_depression_train_imputed = remove_duplicate_columns(all_depression_train_imputed, 'all_depression_train_imputed')\n",
    "all_depression_test_imputed = remove_duplicate_columns(all_depression_test_imputed, 'all_depression_test_imputed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of non-numeric columns with missing values in Train Dataset: 0\n",
      "Mean of missing values per non-numeric column (in %) in Train Dataset: 0.00%\n",
      "Standard deviation of missing values per non-numeric column (in %) in Train Dataset: 0.00%\n",
      "Number of non-numeric columns with missing values in Test Dataset: 0\n",
      "Mean of missing values per non-numeric column (in %) in Test Dataset: 0.00%\n",
      "Standard deviation of missing values per non-numeric column (in %) in Test Dataset: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate missing value statistics for non-numeric columns only and print the results\n",
    "def calculate_missing_values_non_numeric_columns(df, dataset_name, special_columns=None):\n",
    "    if special_columns is None:\n",
    "        special_columns = []\n",
    "    \n",
    "    # Select only the non-numeric columns for calculation, excluding special columns\n",
    "    non_numeric_columns = df.select_dtypes(exclude=['number']).columns.difference(special_columns)\n",
    "    \n",
    "    # Total number of rows in the dataframe\n",
    "    total_rows = len(df)\n",
    "    \n",
    "    # 1. Number of non-numeric columns with missing values\n",
    "    non_numeric_columns_with_missing_values = non_numeric_columns[df[non_numeric_columns].isnull().sum() > 0]\n",
    "    num_non_numeric_columns_with_missing_values = len(non_numeric_columns_with_missing_values)\n",
    "    print(f\"Number of non-numeric columns with missing values in {dataset_name}: {num_non_numeric_columns_with_missing_values}\")\n",
    "    \n",
    "    # 2. Mean and standard deviation of missing values per non-numeric column (in percentage)\n",
    "    missing_values_per_column_percent = df[non_numeric_columns].isnull().sum() / total_rows * 100  # Convert to percentage\n",
    "    mean_missing_values_percent = missing_values_per_column_percent.mean()\n",
    "    std_missing_values_percent = missing_values_per_column_percent.std()\n",
    "    print(f\"Mean of missing values per non-numeric column (in %) in {dataset_name}: {mean_missing_values_percent:.2f}%\")\n",
    "    print(f\"Standard deviation of missing values per non-numeric column (in %) in {dataset_name}: {std_missing_values_percent:.2f}%\")\n",
    "    \n",
    "# Apply the function to both train and test datasets\n",
    "calculate_missing_values_non_numeric_columns(all_depression_train_imputed, \"Train Dataset\", special_columns)\n",
    "calculate_missing_values_non_numeric_columns(all_depression_test_imputed, \"Test Dataset\", special_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Impute missing non-numeric values with the constant \"missing\" for all_depression_train_imputed, ignoring special_columns\n",
    "for column in all_depression_train_imputed.select_dtypes(exclude=['number']).columns:\n",
    "    if column not in special_columns:\n",
    "        all_depression_train_imputed.loc[:, column] = all_depression_train_imputed[column].fillna('missing')\n",
    "\n",
    "# Impute missing non-numeric values with the constant \"missing\" for all_depression_test_imputed, ignoring special_columns\n",
    "for column in all_depression_test_imputed.select_dtypes(exclude=['number']).columns:\n",
    "    if column not in special_columns:\n",
    "        all_depression_test_imputed.loc[:, column] = all_depression_test_imputed[column].fillna('missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data type counts for all_depression_train_imputed:\n",
      "object     485\n",
      "float64    446\n",
      "int64        1\n",
      "dtype: int64\n",
      "\n",
      "Data type counts for all_depression_test_imputed:\n",
      "object            487\n",
      "float64           446\n",
      "datetime64[ns]      2\n",
      "int64               1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Function to get data type counts for a DataFrame\n",
    "def get_data_type_counts(df):\n",
    "    # Get the data types for each column\n",
    "    data_types = df.dtypes\n",
    "    \n",
    "    # Count the occurrences of each data type\n",
    "    data_type_counts = data_types.value_counts()\n",
    "    \n",
    "    return data_type_counts\n",
    "\n",
    "# Apply the function to all imputed datasets\n",
    "data_type_counts_all_train = get_data_type_counts(all_depression_train_imputed)\n",
    "data_type_counts_all_test = get_data_type_counts(all_depression_test_imputed)\n",
    "\n",
    "# Display the counts for each dataset\n",
    "print(\"\\nData type counts for all_depression_train_imputed:\")\n",
    "print(data_type_counts_all_train)\n",
    "\n",
    "print(\"\\nData type counts for all_depression_test_imputed:\")\n",
    "print(data_type_counts_all_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Function to drop specified columns from a DataFrame\n",
    "# those are columns that are accidentally taken from the UK Biobank database with datetime of specific baseline tests\n",
    "# the column birth_date was created to calculate the age at baseline\n",
    "def drop_columns(df, columns_to_drop):\n",
    "    return df.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "# Columns to drop\n",
    "columns_to_drop = ['p4286_i0', 'p4289_i0', 'birth_date']\n",
    "\n",
    "# Apply the function to all imputed datasets\n",
    "all_depression_train_imputed = drop_columns(all_depression_train_imputed, columns_to_drop)\n",
    "all_depression_test_imputed = drop_columns(all_depression_test_imputed, columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing columns in all_depression_train_imputed:\n",
      "Series([], dtype: int64)\n",
      "\n",
      "Missing columns in all_depression_test_imputed:\n",
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "# Function to check for missing values in a DataFrame\n",
    "def check_missing_values(df):\n",
    "    # Check for missing values\n",
    "    missing_values = df.isnull().sum()\n",
    "    \n",
    "    # Display columns with missing values (if any)\n",
    "    missing_columns = missing_values[missing_values > 0]\n",
    "    \n",
    "    return missing_columns\n",
    "\n",
    "# Apply the function to all four imputed datasets\n",
    "missing_columns_all_train = check_missing_values(all_depression_train_imputed)\n",
    "missing_columns_all_test = check_missing_values(all_depression_test_imputed)\n",
    "\n",
    "# Print the results\n",
    "print(\"\\nMissing columns in all_depression_train_imputed:\")\n",
    "print(missing_columns_all_train)\n",
    "\n",
    "print(\"\\nMissing columns in all_depression_test_imputed:\")\n",
    "print(missing_columns_all_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check class balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of 1s in all_depression_train_imputed: 249\n",
      "Number of 0s in all_depression_train_imputed: 251\n",
      "\n",
      "Number of 1s in all_depression_test_imputed: 244\n",
      "Number of 0s in all_depression_test_imputed: 256\n"
     ]
    }
   ],
   "source": [
    "# Function to count the number of 1s and 0s in the 'target' column\n",
    "def count_target_values(df):\n",
    "    count_ones = df['target'].sum()\n",
    "    count_zeros = df['target'].count() - count_ones\n",
    "    \n",
    "    return count_ones, count_zeros\n",
    "\n",
    "# Apply the function to all imputed datasets\n",
    "count_ones_all_train, count_zeros_all_train = count_target_values(all_depression_train_imputed)\n",
    "count_ones_all_test, count_zeros_all_test = count_target_values(all_depression_test_imputed)\n",
    "\n",
    "# Print the results\n",
    "print(f\"\\nNumber of 1s in all_depression_train_imputed: {count_ones_all_train}\")\n",
    "print(f\"Number of 0s in all_depression_train_imputed: {count_zeros_all_train}\")\n",
    "\n",
    "print(f\"\\nNumber of 1s in all_depression_test_imputed: {count_ones_all_test}\")\n",
    "print(f\"Number of 0s in all_depression_test_imputed: {count_zeros_all_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check for outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check for numeric outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range report saved to data/numeric_features_range.csv\n",
      "IQR report saved to data/numeric_features_iqr.csv\n",
      "Outliers report saved to data/numeric_features_outliers.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to calculate range, IQR, and outlier percentage for each numeric feature and save to CSV\n",
    "def generate_range_iqr_and_outlier_reports(df, file_range, file_iqr, file_outliers, special_columns=None):\n",
    "    if special_columns is None:\n",
    "        special_columns = []\n",
    "    \n",
    "    # Select only the numeric columns for calculation, excluding special columns\n",
    "    numeric_columns = df.select_dtypes(include=['number']).columns.difference(special_columns)\n",
    "    \n",
    "    # Calculate Range (Max - Min) for each numeric column\n",
    "    range_data = pd.DataFrame({\n",
    "        'Feature': numeric_columns,\n",
    "        'Range': df[numeric_columns].max() - df[numeric_columns].min()\n",
    "    })\n",
    "    # Sort by Range from most wide to most narrow\n",
    "    range_data_sorted = range_data.sort_values(by='Range', ascending=False)\n",
    "    \n",
    "    # Save the Range report to CSV\n",
    "    range_data_sorted.to_csv(file_range, index=False)\n",
    "    print(f\"Range report saved to {file_range}\")\n",
    "    \n",
    "    # Calculate IQR (Q3 - Q1) for each numeric column\n",
    "    iqr_data = pd.DataFrame({\n",
    "        'Feature': numeric_columns,\n",
    "        'IQR': df[numeric_columns].quantile(0.75) - df[numeric_columns].quantile(0.25)\n",
    "    })\n",
    "    # Sort by IQR from strongest to smallest outlier potential\n",
    "    iqr_data_sorted = iqr_data.sort_values(by='IQR', ascending=False)\n",
    "    \n",
    "    # Save the IQR report to CSV\n",
    "    iqr_data_sorted.to_csv(file_iqr, index=False)\n",
    "    print(f\"IQR report saved to {file_iqr}\")\n",
    "    \n",
    "    # Calculate the percentage of outliers for each numeric column\n",
    "    outlier_data = []\n",
    "    \n",
    "    for column in numeric_columns:\n",
    "        Q1 = df[column].quantile(0.25)\n",
    "        Q3 = df[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        # Count outliers below and above the bounds\n",
    "        below_outliers = (df[column] < lower_bound).sum()\n",
    "        above_outliers = (df[column] > upper_bound).sum()\n",
    "        \n",
    "        total_rows = len(df)\n",
    "        # Calculate percentage of outliers\n",
    "        outlier_percentage = ((below_outliers + above_outliers) / total_rows) * 100\n",
    "        \n",
    "        outlier_data.append({\n",
    "            'Feature': column,\n",
    "            'Outlier Percentage': outlier_percentage\n",
    "        })\n",
    "    \n",
    "    # Create a DataFrame from the outlier data\n",
    "    outlier_data_df = pd.DataFrame(outlier_data)\n",
    "    # Sort by the percentage of outliers from most to least\n",
    "    outlier_data_sorted = outlier_data_df.sort_values(by='Outlier Percentage', ascending=False)\n",
    "    \n",
    "    # Save the Outliers report to CSV\n",
    "    outlier_data_sorted.to_csv(file_outliers, index=False)\n",
    "    print(f\"Outliers report saved to {file_outliers}\")\n",
    "\n",
    "# File paths\n",
    "file_range_csv = 'data/numeric_features_range.csv'\n",
    "file_iqr_csv = 'data/numeric_features_iqr.csv'\n",
    "file_outliers_csv = 'data/numeric_features_outliers.csv'\n",
    "\n",
    "# Generate reports for range, IQR, and outliers\n",
    "generate_range_iqr_and_outlier_reports(all_depression_train_imputed, file_range_csv, file_iqr_csv, file_outliers_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values in all_depression_train_scaled:\n",
      "Series([], dtype: int64)\n",
      "\n",
      "Missing values in all_depression_test_scaled:\n",
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "def z_normalize(train_df, test_df, special_columns=None):\n",
    "    if special_columns is None:\n",
    "        special_columns = []\n",
    "\n",
    "    # Add 'target' column to the list of special columns to be excluded from normalization\n",
    "    columns_to_exclude = special_columns + ['target']\n",
    "\n",
    "    # Filter out special columns and 'target' column from normalization in both train and test datasets\n",
    "    special_columns_in_train = [col for col in columns_to_exclude if col in train_df.columns]\n",
    "    special_columns_in_test = [col for col in columns_to_exclude if col in test_df.columns]\n",
    "    \n",
    "    # Select only the numeric columns for normalization, excluding special columns and 'target'\n",
    "    numeric_columns_train = train_df.select_dtypes(include=['number']).columns.difference(special_columns_in_train)\n",
    "    \n",
    "    # Ensure the test dataframe has the same numeric columns in the same order as the train dataframe\n",
    "    numeric_columns_test = numeric_columns_train.intersection(test_df.columns)\n",
    "    \n",
    "    # Initialize the StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Fit the scaler on the numeric training data and transform the training data\n",
    "    train_df_scaled = train_df.copy()\n",
    "    train_df_scaled[numeric_columns_train] = scaler.fit_transform(train_df[numeric_columns_train])\n",
    "    \n",
    "    # Transform the numeric test data using the scaler fitted on the training data\n",
    "    test_df_scaled = test_df.copy()\n",
    "    test_df_scaled[numeric_columns_test] = scaler.transform(test_df[numeric_columns_test])\n",
    "\n",
    "    # Ensure non-numeric and special columns (including 'target') are retained in the test dataset\n",
    "    train_df_scaled[special_columns_in_train] = train_df[special_columns_in_train]\n",
    "    test_df_scaled[special_columns_in_test] = test_df[special_columns_in_test]\n",
    "    \n",
    "    return train_df_scaled, test_df_scaled\n",
    "\n",
    "# Apply Z-normalization to the datasets using the function\n",
    "all_depression_train_scaled, all_depression_test_scaled = z_normalize(all_depression_train_imputed, all_depression_test_imputed, special_columns)\n",
    "\n",
    "# Function to check for missing values, excluding special columns and 'target'\n",
    "def check_missing_values(df, special_columns=None):\n",
    "    if special_columns is None:\n",
    "        special_columns = []\n",
    "    \n",
    "    # Filter out special columns and 'target' column from the check\n",
    "    columns_to_check = df.columns.difference(special_columns + ['target'])\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_values = df[columns_to_check].isnull().sum()\n",
    "    missing_values = missing_values[missing_values > 0]  # Filter only columns with missing values\n",
    "    \n",
    "    return missing_values\n",
    "\n",
    "# Check for missing values in all datasets\n",
    "missing_values_all_train = check_missing_values(all_depression_train_scaled, special_columns)\n",
    "missing_values_all_test = check_missing_values(all_depression_test_scaled, special_columns)\n",
    "\n",
    "print(\"\\nMissing values in all_depression_train_scaled:\")\n",
    "print(missing_values_all_train)\n",
    "\n",
    "print(\"\\nMissing values in all_depression_test_scaled:\")\n",
    "print(missing_values_all_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## check for non-numeric outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'all_depression_train_scaled' has 0 columns with unique string entries.\n",
      "The total number of unique string entries in 'all_depression_train_scaled' is 0.\n",
      "Mean number of unique string entries per non-numeric feature in 'all_depression_train_scaled' is 0.00.\n",
      "Standard deviation of unique string entries per non-numeric feature in 'all_depression_train_scaled' is 0.00.\n",
      "\n",
      "Dataset 'all_depression_test_scaled' has 0 columns with unique string entries.\n",
      "The total number of unique string entries in 'all_depression_test_scaled' is 0.\n",
      "Mean number of unique string entries per non-numeric feature in 'all_depression_test_scaled' is 0.00.\n",
      "Standard deviation of unique string entries per non-numeric feature in 'all_depression_test_scaled' is 0.00.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to count unique string occurrences in non-numeric columns and calculate mean and standard deviation\n",
    "def count_unique_strings_with_stats(df, special_columns=None):\n",
    "    if special_columns is None:\n",
    "        special_columns = []\n",
    "    \n",
    "    unique_string_counts = []\n",
    "    total_unique_count = 0\n",
    "\n",
    "    # Filter out special columns that may no longer exist in the dataset\n",
    "    special_columns_in_df = [col for col in special_columns if col in df.columns]\n",
    "    \n",
    "    # Iterate through each non-numeric column in the DataFrame, excluding special columns\n",
    "    non_numeric_columns = df.select_dtypes(exclude=np.number).columns.difference(special_columns_in_df)\n",
    "    \n",
    "    for column in non_numeric_columns:\n",
    "        # Get the counts of each unique value in the column\n",
    "        value_counts = df[column].value_counts()\n",
    "        \n",
    "        # Count how many times a unique value occurs only once\n",
    "        count_unique = (value_counts == 1).sum()\n",
    "        \n",
    "        # Store the count in the list and add to total\n",
    "        unique_string_counts.append(count_unique)\n",
    "        total_unique_count += count_unique\n",
    "\n",
    "    # Calculate mean and standard deviation for the unique string counts\n",
    "    mean_unique_count = np.mean(unique_string_counts)\n",
    "    std_unique_count = np.std(unique_string_counts)\n",
    "\n",
    "    # Count how many columns have at least one unique string occurrence\n",
    "    columns_with_unique_entries = sum(count > 0 for count in unique_string_counts)\n",
    "    \n",
    "    return columns_with_unique_entries, total_unique_count, mean_unique_count, std_unique_count\n",
    "\n",
    "# Apply the function to all scaled datasets using special_columns\n",
    "unique_string_stats_train = count_unique_strings_with_stats(all_depression_train_scaled, special_columns)\n",
    "unique_string_stats_test = count_unique_strings_with_stats(all_depression_test_scaled, special_columns)\n",
    "\n",
    "# Adjusted print statements\n",
    "datasets_info = [\n",
    "    (\"all_depression_train_scaled\", unique_string_stats_train),\n",
    "    (\"all_depression_test_scaled\", unique_string_stats_test)\n",
    "]\n",
    "\n",
    "for dataset_name, (columns_with_unique_entries, total_unique_count, mean_unique_count, std_unique_count) in datasets_info:\n",
    "    print(f\"Dataset '{dataset_name}' has {columns_with_unique_entries} columns with unique string entries.\")\n",
    "    print(f\"The total number of unique string entries in '{dataset_name}' is {total_unique_count}.\")\n",
    "    print(f\"Mean number of unique string entries per non-numeric feature in '{dataset_name}' is {mean_unique_count:.2f}.\")\n",
    "    print(f\"Standard deviation of unique string entries per non-numeric feature in '{dataset_name}' is {std_unique_count:.2f}.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### replace all non-numeric values that only occur once in a column with the most frequent value in this column for all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Function to replace unique entries with the most frequent entry in non-numeric columns\n",
    "def replace_unique_with_most_frequent(df, special_columns=None):\n",
    "    if special_columns is None:\n",
    "        special_columns = []\n",
    "    \n",
    "    # Filter out special columns that may no longer exist in the dataset\n",
    "    special_columns_in_df = [col for col in special_columns if col in df.columns]\n",
    "    \n",
    "    # Iterate through each non-numeric column in the DataFrame, excluding special columns\n",
    "    non_numeric_columns = df.select_dtypes(exclude=np.number).columns.difference(special_columns_in_df)\n",
    "    \n",
    "    for column in non_numeric_columns:\n",
    "        # Get the value counts of the column\n",
    "        value_counts = df[column].value_counts()\n",
    "        \n",
    "        # Identify the most frequent entry\n",
    "        most_frequent_entry = value_counts.idxmax()\n",
    "        \n",
    "        # Replace all unique entries with the most frequent entry\n",
    "        df[column] = df[column].apply(\n",
    "            lambda x: most_frequent_entry if value_counts[x] == 1 else x\n",
    "        )\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply the function to all scaled datasets excluding special_columns\n",
    "all_depression_train_scaled = replace_unique_with_most_frequent(all_depression_train_scaled, special_columns)\n",
    "all_depression_test_scaled = replace_unique_with_most_frequent(all_depression_test_scaled, special_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## one hot encode all non-numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "all_depression_train_encoded head (drop_first=False):\n",
      "          p34    p46_i0    p47_i0    p48_i0    p49_i0    p50_i0    p51_i0  \\\n",
      "126 -0.628473  0.863474 -0.039786  1.415492  0.650315  0.079338  0.292360   \n",
      "314 -0.522276  1.593990 -1.079494 -1.273047 -1.216467 -1.596765  1.177561   \n",
      "267  1.672459  0.323715 -0.994850 -1.431662 -1.561600  1.170085 -0.684851   \n",
      "282  0.221102 -0.502104  1.059365 -1.196499  1.378711  1.261222  0.025512   \n",
      "496 -1.194857  1.405302 -0.489943  1.290280  1.238361  1.614704 -1.639406   \n",
      "\n",
      "       p68_i0    p74_i0    p77_i0  ...  p23075_i0_Category_A  \\\n",
      "126  0.932815  1.262318  1.308699  ...                     0   \n",
      "314 -1.601980 -0.092063  1.037718  ...                     0   \n",
      "267  0.670705 -0.518523  0.025388  ...                     0   \n",
      "282 -1.183141  0.225843 -0.151887  ...                     0   \n",
      "496  1.491095  0.251065 -1.756568  ...                     1   \n",
      "\n",
      "     p23075_i0_Category_B  p23075_i0_Category_C  p23075_i0_Category_D  \\\n",
      "126                     0                     0                     0   \n",
      "314                     0                     0                     0   \n",
      "267                     0                     1                     0   \n",
      "282                     0                     0                     0   \n",
      "496                     0                     0                     0   \n",
      "\n",
      "     p23075_i0_Category_E  p23165_Category_A  p23165_Category_B  \\\n",
      "126                     1                  0                  0   \n",
      "314                     1                  0                  1   \n",
      "267                     0                  0                  1   \n",
      "282                     1                  1                  0   \n",
      "496                     0                  0                  1   \n",
      "\n",
      "     p23165_Category_C  p23165_Category_D  p23165_Category_E  \n",
      "126                  0                  0                  1  \n",
      "314                  0                  0                  0  \n",
      "267                  0                  0                  0  \n",
      "282                  0                  0                  0  \n",
      "496                  0                  0                  0  \n",
      "\n",
      "[5 rows x 2857 columns]\n",
      "\n",
      "all_depression_test_encoded head (drop_first=False):\n",
      "          p34    p46_i0    p47_i0    p48_i0    p49_i0    p50_i0    p51_i0  \\\n",
      "126  1.389268  1.618851  1.526657  0.333701  1.704100  0.858359 -0.890921   \n",
      "314 -1.230256 -1.586627  0.433422 -1.316499  0.801222  0.037118 -1.417916   \n",
      "267  0.114905 -1.612564 -0.976938 -0.111463 -1.470884 -0.451586 -0.213269   \n",
      "282 -1.725841  0.540942 -1.009980 -0.701040  0.191855  0.073862 -0.699467   \n",
      "496  0.433496  0.606634 -1.746382  1.262737 -0.694624  1.150897  0.815795   \n",
      "\n",
      "       p68_i0    p74_i0    p77_i0  ...  p23075_i0_Category_E  \\\n",
      "126  1.271055  1.588721  0.699740  ...                     1   \n",
      "314 -0.212186 -0.564753 -1.113061  ...                     1   \n",
      "267  1.670376 -0.971329 -0.028762  ...                     0   \n",
      "282 -1.137874  0.505007  0.731484  ...                     1   \n",
      "496 -0.564778 -0.886060 -1.205523  ...                     1   \n",
      "\n",
      "     p23165_Category_A  p23165_Category_B  p23165_Category_C  \\\n",
      "126                  1                  0                  0   \n",
      "314                  0                  0                  0   \n",
      "267                  0                  0                  1   \n",
      "282                  0                  1                  0   \n",
      "496                  0                  0                  0   \n",
      "\n",
      "     p23165_Category_D  p23165_Category_E     eid    p130894     p130895  \\\n",
      "126                  0                  0  ID_127 2015-12-14  Category_C   \n",
      "314                  0                  1  ID_315 2020-11-05  Category_D   \n",
      "267                  0                  0  ID_268 2022-08-15  Category_D   \n",
      "282                  0                  0  ID_283 2019-01-28  Category_A   \n",
      "496                  0                  1  ID_497 2007-10-02  Category_C   \n",
      "\n",
      "        p53_i0  \n",
      "126 2008-03-26  \n",
      "314 2008-01-21  \n",
      "267 2007-06-12  \n",
      "282 2010-09-07  \n",
      "496 2008-01-26  \n",
      "\n",
      "[5 rows x 2861 columns]\n",
      "\n",
      "all_depression_train_encoded_drop head (drop_first=True):\n",
      "          p34    p46_i0    p47_i0    p48_i0    p49_i0    p50_i0    p51_i0  \\\n",
      "126 -0.628473  0.863474 -0.039786  1.415492  0.650315  0.079338  0.292360   \n",
      "314 -0.522276  1.593990 -1.079494 -1.273047 -1.216467 -1.596765  1.177561   \n",
      "267  1.672459  0.323715 -0.994850 -1.431662 -1.561600  1.170085 -0.684851   \n",
      "282  0.221102 -0.502104  1.059365 -1.196499  1.378711  1.261222  0.025512   \n",
      "496 -1.194857  1.405302 -0.489943  1.290280  1.238361  1.614704 -1.639406   \n",
      "\n",
      "       p68_i0    p74_i0    p77_i0  ...  p23074_i0_Category_D  \\\n",
      "126  0.932815  1.262318  1.308699  ...                     0   \n",
      "314 -1.601980 -0.092063  1.037718  ...                     0   \n",
      "267  0.670705 -0.518523  0.025388  ...                     0   \n",
      "282 -1.183141  0.225843 -0.151887  ...                     0   \n",
      "496  1.491095  0.251065 -1.756568  ...                     0   \n",
      "\n",
      "     p23074_i0_Category_E  p23075_i0_Category_B  p23075_i0_Category_C  \\\n",
      "126                     0                     0                     0   \n",
      "314                     0                     0                     0   \n",
      "267                     1                     0                     1   \n",
      "282                     1                     0                     0   \n",
      "496                     0                     0                     0   \n",
      "\n",
      "     p23075_i0_Category_D  p23075_i0_Category_E  p23165_Category_B  \\\n",
      "126                     0                     1                  0   \n",
      "314                     0                     1                  1   \n",
      "267                     0                     0                  1   \n",
      "282                     0                     1                  0   \n",
      "496                     0                     0                  1   \n",
      "\n",
      "     p23165_Category_C  p23165_Category_D  p23165_Category_E  \n",
      "126                  0                  0                  1  \n",
      "314                  0                  0                  0  \n",
      "267                  0                  0                  0  \n",
      "282                  0                  0                  0  \n",
      "496                  0                  0                  0  \n",
      "\n",
      "[5 rows x 2375 columns]\n",
      "\n",
      "all_depression_test_encoded_drop head (drop_first=True):\n",
      "          p34    p46_i0    p47_i0    p48_i0    p49_i0    p50_i0    p51_i0  \\\n",
      "126  1.389268  1.618851  1.526657  0.333701  1.704100  0.858359 -0.890921   \n",
      "314 -1.230256 -1.586627  0.433422 -1.316499  0.801222  0.037118 -1.417916   \n",
      "267  0.114905 -1.612564 -0.976938 -0.111463 -1.470884 -0.451586 -0.213269   \n",
      "282 -1.725841  0.540942 -1.009980 -0.701040  0.191855  0.073862 -0.699467   \n",
      "496  0.433496  0.606634 -1.746382  1.262737 -0.694624  1.150897  0.815795   \n",
      "\n",
      "       p68_i0    p74_i0    p77_i0  ...  p23075_i0_Category_D  \\\n",
      "126  1.271055  1.588721  0.699740  ...                     0   \n",
      "314 -0.212186 -0.564753 -1.113061  ...                     0   \n",
      "267  1.670376 -0.971329 -0.028762  ...                     0   \n",
      "282 -1.137874  0.505007  0.731484  ...                     0   \n",
      "496 -0.564778 -0.886060 -1.205523  ...                     0   \n",
      "\n",
      "     p23075_i0_Category_E  p23165_Category_B  p23165_Category_C  \\\n",
      "126                     1                  0                  0   \n",
      "314                     1                  0                  0   \n",
      "267                     0                  0                  1   \n",
      "282                     1                  1                  0   \n",
      "496                     1                  0                  0   \n",
      "\n",
      "     p23165_Category_D  p23165_Category_E     eid    p130894     p130895  \\\n",
      "126                  0                  0  ID_127 2015-12-14  Category_C   \n",
      "314                  0                  1  ID_315 2020-11-05  Category_D   \n",
      "267                  0                  0  ID_268 2022-08-15  Category_D   \n",
      "282                  0                  0  ID_283 2019-01-28  Category_A   \n",
      "496                  0                  1  ID_497 2007-10-02  Category_C   \n",
      "\n",
      "        p53_i0  \n",
      "126 2008-03-26  \n",
      "314 2008-01-21  \n",
      "267 2007-06-12  \n",
      "282 2010-09-07  \n",
      "496 2008-01-26  \n",
      "\n",
      "[5 rows x 2379 columns]\n"
     ]
    }
   ],
   "source": [
    "# Function to one-hot encode the training set and ensure the test set has the same columns\n",
    "def one_hot_encode_train_test(train_df, test_df, special_columns=None, drop_first=False):\n",
    "    if special_columns is None:\n",
    "        special_columns = []\n",
    "    \n",
    "    # Filter out special columns that may no longer exist in the dataset\n",
    "    special_columns_in_train = [col for col in special_columns if col in train_df.columns]\n",
    "    special_columns_in_test = [col for col in special_columns if col in test_df.columns]\n",
    "    \n",
    "    # Separate the columns to exclude (special columns) from the rest of the DataFrame\n",
    "    train_to_encode = train_df.drop(columns=special_columns_in_train)\n",
    "    test_to_encode = test_df.drop(columns=special_columns_in_test)\n",
    "    \n",
    "    # One-hot encode the training set\n",
    "    train_encoded = pd.get_dummies(train_to_encode, drop_first=drop_first)\n",
    "    \n",
    "    # One-hot encode the test set\n",
    "    test_encoded = pd.get_dummies(test_to_encode, drop_first=drop_first)\n",
    "    \n",
    "    # Align the test set columns to match the training set columns\n",
    "    test_encoded = test_encoded.reindex(columns=train_encoded.columns, fill_value=0)\n",
    "    \n",
    "    # Combine the encoded DataFrames with the special columns at the end\n",
    "    train_final = pd.concat([train_encoded, train_df[special_columns_in_train]], axis=1)\n",
    "    test_final = pd.concat([test_encoded, test_df[special_columns_in_test]], axis=1)\n",
    "    \n",
    "    return train_final, test_final\n",
    "\n",
    "# Apply the function to both options (drop_first=False and drop_first=True)\n",
    "all_depression_train_encoded, all_depression_test_encoded = one_hot_encode_train_test(\n",
    "    all_depression_train_scaled, all_depression_test_scaled, special_columns, drop_first=False)\n",
    "\n",
    "all_depression_train_encoded_drop, all_depression_test_encoded_drop = one_hot_encode_train_test(\n",
    "    all_depression_train_scaled, all_depression_test_scaled, special_columns, drop_first=True)\n",
    "\n",
    "# Print the head of each encoded DataFrame to verify the changes\n",
    "print(\"\\nall_depression_train_encoded head (drop_first=False):\")\n",
    "print(all_depression_train_encoded.head())\n",
    "\n",
    "print(\"\\nall_depression_test_encoded head (drop_first=False):\")\n",
    "print(all_depression_test_encoded.head())\n",
    "\n",
    "print(\"\\nall_depression_train_encoded_drop head (drop_first=True):\")\n",
    "print(all_depression_train_encoded_drop.head())\n",
    "\n",
    "print(\"\\nall_depression_test_encoded_drop head (drop_first=True):\")\n",
    "print(all_depression_test_encoded_drop.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clean column names to avoid problems in modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Function to clean column names\n",
    "def clean_column_name(name):\n",
    "    # Replace spaces with underscores\n",
    "    name = name.replace(' ', '_')\n",
    "    # Remove any character that is not a letter, digit, or underscore\n",
    "    name = re.sub(r'[^\\w]', '', name)\n",
    "    return name\n",
    "\n",
    "# Function to clean column names in a DataFrame, excluding special columns\n",
    "def clean_column_names(df, special_columns=None):\n",
    "    if special_columns is None:\n",
    "        special_columns = []\n",
    "    \n",
    "    # Filter out special columns that may no longer exist in the dataset\n",
    "    special_columns_in_df = df.columns.intersection(special_columns)\n",
    "    \n",
    "    # Clean only the columns that are not in the special columns list\n",
    "    columns_to_clean = df.columns.difference(special_columns_in_df)\n",
    "    cleaned_columns = {col: clean_column_name(col) for col in columns_to_clean}\n",
    "    \n",
    "    # Rename the columns in the DataFrame\n",
    "    df.rename(columns=cleaned_columns, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply the function to all four encoded datasets\n",
    "all_depression_train_encoded = clean_column_names(all_depression_train_encoded, special_columns)\n",
    "all_depression_test_encoded = clean_column_names(all_depression_test_encoded, special_columns)\n",
    "all_depression_train_encoded_drop = clean_column_names(all_depression_train_encoded_drop, special_columns)\n",
    "all_depression_test_encoded_drop = clean_column_names(all_depression_test_encoded_drop, special_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set at the end all colums to datatype float 64 except target to int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Function to set 'target' column to int64 and all other columns to float64, excluding special columns\n",
    "def set_column_types(df, target_column='target', special_columns=None):\n",
    "    if special_columns is None:\n",
    "        special_columns = []\n",
    "    \n",
    "    # Set the target column to int64\n",
    "    if target_column in df.columns:\n",
    "        df[target_column] = df[target_column].astype('int64')\n",
    "    \n",
    "    # Set all other columns to float64, excluding the target and special columns\n",
    "    for column in df.columns:\n",
    "        if column != target_column and column not in special_columns:\n",
    "            df[column] = df[column].astype('float64')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply the function to all four datasets\n",
    "all_depression_train_encoded = set_column_types(all_depression_train_encoded, special_columns=special_columns)\n",
    "all_depression_test_encoded = set_column_types(all_depression_test_encoded, special_columns=special_columns)\n",
    "all_depression_train_encoded_drop = set_column_types(all_depression_train_encoded_drop, special_columns=special_columns)\n",
    "all_depression_test_encoded_drop = set_column_types(all_depression_test_encoded_drop, special_columns=special_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type counts in all_depression_train_encoded (drop_first=False):\n",
      "float64    2856\n",
      "int64         1\n",
      "dtype: int64\n",
      "\n",
      "Data type counts in all_depression_test_encoded (drop_first=False):\n",
      "float64           2856\n",
      "object               2\n",
      "datetime64[ns]       2\n",
      "int64                1\n",
      "dtype: int64\n",
      "\n",
      "Data type counts in all_depression_train_encoded_drop (drop_first=True):\n",
      "float64    2374\n",
      "int64         1\n",
      "dtype: int64\n",
      "\n",
      "Data type counts in all_depression_test_encoded_drop (drop_first=True):\n",
      "float64           2374\n",
      "object               2\n",
      "datetime64[ns]       2\n",
      "int64                1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Function to count the number of columns for each data type in a DataFrame\n",
    "def count_datatype_columns(df):\n",
    "    return df.dtypes.value_counts()\n",
    "\n",
    "# Apply the function to all four datasets and store the results\n",
    "datatype_counts_train_encoded = count_datatype_columns(all_depression_train_encoded)\n",
    "datatype_counts_test_encoded = count_datatype_columns(all_depression_test_encoded)\n",
    "datatype_counts_train_encoded_drop = count_datatype_columns(all_depression_train_encoded_drop)\n",
    "datatype_counts_test_encoded_drop = count_datatype_columns(all_depression_test_encoded_drop)\n",
    "\n",
    "# Display the counts for each dataset\n",
    "print(\"Data type counts in all_depression_train_encoded (drop_first=False):\")\n",
    "print(datatype_counts_train_encoded)\n",
    "\n",
    "print(\"\\nData type counts in all_depression_test_encoded (drop_first=False):\")\n",
    "print(datatype_counts_test_encoded)\n",
    "\n",
    "print(\"\\nData type counts in all_depression_train_encoded_drop (drop_first=True):\")\n",
    "print(datatype_counts_train_encoded_drop)\n",
    "\n",
    "print(\"\\nData type counts in all_depression_test_encoded_drop (drop_first=True):\")\n",
    "print(datatype_counts_test_encoded_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with missing values in all_depression_train_encoded (drop_first=False):\n",
      "Series([], dtype: int64)\n",
      "\n",
      "Columns with missing values in all_depression_test_encoded (drop_first=False):\n",
      "Series([], dtype: int64)\n",
      "\n",
      "Columns with missing values in all_depression_train_encoded_drop (drop_first=True):\n",
      "Series([], dtype: int64)\n",
      "\n",
      "Columns with missing values in all_depression_test_encoded_drop (drop_first=True):\n",
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "# Function to check for missing values in a DataFrame, excluding special columns\n",
    "def check_missing_values(df, special_columns=None):\n",
    "    if special_columns is None:\n",
    "        special_columns = []\n",
    "    \n",
    "    # Filter out special columns that may no longer exist in the dataset\n",
    "    columns_to_check = df.columns.difference(special_columns)\n",
    "    \n",
    "    # Check for missing values in the remaining columns\n",
    "    missing_values = df[columns_to_check].isnull().sum()\n",
    "    \n",
    "    return missing_values[missing_values > 0]\n",
    "\n",
    "# Apply the function to all four datasets and store the results\n",
    "missing_values_train_encoded = check_missing_values(all_depression_train_encoded, special_columns)\n",
    "missing_values_test_encoded = check_missing_values(all_depression_test_encoded, special_columns)\n",
    "missing_values_train_encoded_drop = check_missing_values(all_depression_train_encoded_drop, special_columns)\n",
    "missing_values_test_encoded_drop = check_missing_values(all_depression_test_encoded_drop, special_columns)\n",
    "\n",
    "# Display the columns with missing values for each dataset\n",
    "print(\"Columns with missing values in all_depression_train_encoded (drop_first=False):\")\n",
    "print(missing_values_train_encoded)\n",
    "\n",
    "print(\"\\nColumns with missing values in all_depression_test_encoded (drop_first=False):\")\n",
    "print(missing_values_test_encoded)\n",
    "\n",
    "print(\"\\nColumns with missing values in all_depression_train_encoded_drop (drop_first=True):\")\n",
    "print(missing_values_train_encoded_drop)\n",
    "\n",
    "print(\"\\nColumns with missing values in all_depression_test_encoded_drop (drop_first=True):\")\n",
    "print(missing_values_test_encoded_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# store data to continue in a separate notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save each of the four datasets as a .pkl file in the \"data\" folder with the term \"late\" and the size appended\n",
    "all_depression_train_encoded.to_pickle(f\"data/all_late_depression_train_encoded_{all_depression_train_size}.pkl\")\n",
    "all_depression_test_encoded.to_pickle(f\"data/all_late_depression_test_encoded_{all_depression_test_size}.pkl\")\n",
    "all_depression_train_encoded_drop.to_pickle(f\"data/all_late_depression_train_encoded_drop_{all_depression_train_size}.pkl\")\n",
    "all_depression_test_encoded_drop.to_pickle(f\"data/all_late_depression_test_encoded_drop_{all_depression_test_size}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FRP311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
