{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dxdata\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the \"feature_selection\" subfolder if it doesn't already exist\n",
    "os.makedirs(\"feature_selection\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load the all_depression training and testing DataFrames from the encoded pickle files\n",
    "train_df_all = pd.read_pickle(\"data/all_late_depression_train_encoded_500.pkl\")\n",
    "test_df_all = pd.read_pickle(\"data/all_late_depression_test_encoded_500.pkl\")\n",
    "\n",
    "# Load the all_depression training and testing DataFrames with drop_first=True (encoded_drop)\n",
    "train_df_all_drop = pd.read_pickle(\"data/all_late_depression_train_encoded_drop_500.pkl\")\n",
    "test_df_all_drop = pd.read_pickle(\"data/all_late_depression_test_encoded_drop_500.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# List of special columns only available in the test dataset\n",
    "special_columns = ['eid', 'p130894', 'p130895', 'p53_i0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# columns with polygenetic risk scores (PRS) from UK Biobank\n",
    "# columns with too many missing values that have therefore been removed by data cleaning are already excluded\n",
    "PRS_columns = [\n",
    "    \"p26202\", \"p26204\", \"p26206\", \"p26210\", \"p26212\", \"p26214\", \"p26216\", \n",
    "    \"p26218\", \"p26220\", \"p26223\", \"p26225\", \"p26227\", \"p26229\", \"p26232\", \n",
    "    \"p26234\", \"p26238\", \"p26240\", \"p26242\", \"p26244\", \"p26246\", \"p26248\", \n",
    "    \"p26250\", \"p26252\", \"p26254\", \"p26258\", \"p26260\", \"p26265\", \"p26267\", \n",
    "    \"p26269\", \"p26273\", \"p26275\", \"p26278\", \"p26283\", \"p26285\", \"p26287\", \n",
    "    \"p26289\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Head of train_df_all (drop_first=False):\n",
      "          p34    p46_i0    p47_i0    p48_i0    p49_i0    p50_i0   p51_i0  \\\n",
      "126  0.808156 -1.366783 -0.540316 -1.425344  1.009273  1.602138 -1.28724   \n",
      "\n",
      "       p68_i0    p74_i0    p77_i0  ...  p23075_i0_Category_A  \\\n",
      "126 -1.688764 -1.349758  0.594215  ...                   0.0   \n",
      "\n",
      "     p23075_i0_Category_B  p23075_i0_Category_C  p23075_i0_Category_D  \\\n",
      "126                   0.0                   0.0                   1.0   \n",
      "\n",
      "     p23075_i0_Category_E  p23165_Category_A  p23165_Category_B  \\\n",
      "126                   0.0                1.0                0.0   \n",
      "\n",
      "     p23165_Category_C  p23165_Category_D  p23165_Category_E  \n",
      "126                0.0                0.0                0.0  \n",
      "\n",
      "[1 rows x 2857 columns]\n",
      "\n",
      "Head of test_df_all (drop_first=False):\n",
      "          p34    p46_i0    p47_i0    p48_i0   p49_i0    p50_i0    p51_i0  \\\n",
      "126 -0.552496 -1.518567 -0.339135  0.744709  0.79908 -0.082552 -0.834985   \n",
      "\n",
      "       p68_i0    p74_i0    p77_i0  ...  p23075_i0_Category_E  \\\n",
      "126  0.474047 -1.297782 -1.920003  ...                   0.0   \n",
      "\n",
      "     p23165_Category_A  p23165_Category_B  p23165_Category_C  \\\n",
      "126                0.0                0.0                0.0   \n",
      "\n",
      "     p23165_Category_D  p23165_Category_E     eid    p130894     p130895  \\\n",
      "126                0.0                1.0  ID_127 2016-04-05  Category_B   \n",
      "\n",
      "        p53_i0  \n",
      "126 2008-10-21  \n",
      "\n",
      "[1 rows x 2861 columns]\n",
      "\n",
      "Head of train_df_all_drop (drop_first=True):\n",
      "          p34    p46_i0    p47_i0    p48_i0    p49_i0    p50_i0   p51_i0  \\\n",
      "126  0.808156 -1.366783 -0.540316 -1.425344  1.009273  1.602138 -1.28724   \n",
      "\n",
      "       p68_i0    p74_i0    p77_i0  ...  p23074_i0_Category_D  \\\n",
      "126 -1.688764 -1.349758  0.594215  ...                   0.0   \n",
      "\n",
      "     p23074_i0_Category_E  p23075_i0_Category_B  p23075_i0_Category_C  \\\n",
      "126                   0.0                   0.0                   0.0   \n",
      "\n",
      "     p23075_i0_Category_D  p23075_i0_Category_E  p23165_Category_B  \\\n",
      "126                   1.0                   0.0                0.0   \n",
      "\n",
      "     p23165_Category_C  p23165_Category_D  p23165_Category_E  \n",
      "126                0.0                0.0                0.0  \n",
      "\n",
      "[1 rows x 2375 columns]\n",
      "\n",
      "Head of test_df_all_drop (drop_first=True):\n",
      "          p34    p46_i0    p47_i0    p48_i0   p49_i0    p50_i0    p51_i0  \\\n",
      "126 -0.552496 -1.518567 -0.339135  0.744709  0.79908 -0.082552 -0.834985   \n",
      "\n",
      "       p68_i0    p74_i0    p77_i0  ...  p23075_i0_Category_D  \\\n",
      "126  0.474047 -1.297782 -1.920003  ...                   1.0   \n",
      "\n",
      "     p23075_i0_Category_E  p23165_Category_B  p23165_Category_C  \\\n",
      "126                   0.0                0.0                0.0   \n",
      "\n",
      "     p23165_Category_D  p23165_Category_E     eid    p130894     p130895  \\\n",
      "126                0.0                1.0  ID_127 2016-04-05  Category_B   \n",
      "\n",
      "        p53_i0  \n",
      "126 2008-10-21  \n",
      "\n",
      "[1 rows x 2379 columns]\n"
     ]
    }
   ],
   "source": [
    "# Print the first row (head) for all four datasets\n",
    "\n",
    "print(\"\\nHead of train_df_all (drop_first=False):\")\n",
    "print(train_df_all.head(1))\n",
    "\n",
    "print(\"\\nHead of test_df_all (drop_first=False):\")\n",
    "print(test_df_all.head(1))\n",
    "\n",
    "print(\"\\nHead of train_df_all_drop (drop_first=True):\")\n",
    "print(train_df_all_drop.head(1))\n",
    "\n",
    "print(\"\\nHead of test_df_all_drop (drop_first=True):\")\n",
    "print(test_df_all_drop.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature selection\n",
    "- mutual information\n",
    "- logistic regression with lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mutual information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate mutual information for each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Mutual Information Features for train_df_all (drop_first=False):\n",
      "                  Feature  Mutual Information\n",
      "714   p1210_i0_Category_D            0.091295\n",
      "807   p1418_i0_Category_B            0.085663\n",
      "764   p1319_i0_Category_D            0.078695\n",
      "2310  p6152_i0_Category_E            0.078680\n",
      "935   p1697_i0_Category_E            0.078249\n",
      "1800  p4136_i0_Category_E            0.076601\n",
      "1664  p3773_i0_Category_D            0.076288\n",
      "1382  p2867_i0_Category_B            0.073433\n",
      "682   p1150_i0_Category_B            0.073087\n",
      "1177  p2277_i0_Category_B            0.072446\n",
      "\n",
      "Top 10 Mutual Information Features for train_df_all_drop (drop_first=True):\n",
      "                   Feature  Mutual Information\n",
      "1237   p2976_i0_Category_E            0.085964\n",
      "1529   p4136_i0_Category_E            0.083729\n",
      "2257  p22684_i0_Category_E            0.076839\n",
      "1401   p3720_i0_Category_E            0.072947\n",
      "1393   p3700_i0_Category_E            0.072005\n",
      "1722   p5001_i0_Category_B            0.071827\n",
      "264              p23478_i0            0.069837\n",
      "1137   p2704_i0_Category_E            0.069018\n",
      "1031   p2277_i0_Category_C            0.068363\n",
      "2200  p20123_i0_Category_D            0.067235\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate mutual information for all features and return the complete DataFrame\n",
    "def calculate_mutual_info(train_df, target_column):\n",
    "    # Separate features and target\n",
    "    X = train_df.drop(columns=[target_column])  # Features\n",
    "    y = train_df[target_column]  # Target variable\n",
    "\n",
    "    # Calculate mutual information between each feature and the target\n",
    "    mutual_info = mutual_info_classif(X, y, random_state=81)\n",
    "\n",
    "    # Create a DataFrame to display the mutual information scores\n",
    "    mutual_info_df = pd.DataFrame({'Feature': X.columns, 'Mutual Information': mutual_info})\n",
    "\n",
    "    # Sort the features by their mutual information scores in descending order\n",
    "    mutual_info_df.sort_values(by='Mutual Information', ascending=False, inplace=True)\n",
    "\n",
    "    # Print the top 10 features by mutual information scores\n",
    "    print(mutual_info_df.head(10))\n",
    "\n",
    "    # Return the complete DataFrame with mutual information scores for all features\n",
    "    return mutual_info_df\n",
    "\n",
    "# Calculate and store mutual information for both datasets\n",
    "\n",
    "# For train_df_all (drop_first=False)\n",
    "print(\"Top 10 Mutual Information Features for train_df_all (drop_first=False):\")\n",
    "mutual_info_all = calculate_mutual_info(train_df_all, 'target')\n",
    "\n",
    "# For train_df_all_drop (drop_first=True)\n",
    "print(\"\\nTop 10 Mutual Information Features for train_df_all_drop (drop_first=True):\")\n",
    "mutual_info_all_drop = calculate_mutual_info(train_df_all_drop, 'target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### select different features sizes with highest mutual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train_df_all (drop_first=False):\n",
      "Feature size: 5 for train_df_all\n",
      "Shape of train_df_all_features_5: (500, 6)\n",
      "Feature size: 10 for train_df_all\n",
      "Shape of train_df_all_features_10: (500, 11)\n",
      "Feature size: 15 for train_df_all\n",
      "Shape of train_df_all_features_15: (500, 16)\n",
      "Feature size: 20 for train_df_all\n",
      "Shape of train_df_all_features_20: (500, 21)\n",
      "Feature size: 30 for train_df_all\n",
      "Shape of train_df_all_features_30: (500, 31)\n",
      "Feature size: 50 for train_df_all\n",
      "Shape of train_df_all_features_50: (500, 51)\n",
      "\n",
      "Processing train_df_all_drop (drop_first=True):\n",
      "Feature size: 5 for train_df_all_drop\n",
      "Shape of train_df_all_drop_features_5: (500, 6)\n",
      "Feature size: 10 for train_df_all_drop\n",
      "Shape of train_df_all_drop_features_10: (500, 11)\n",
      "Feature size: 15 for train_df_all_drop\n",
      "Shape of train_df_all_drop_features_15: (500, 16)\n",
      "Feature size: 20 for train_df_all_drop\n",
      "Shape of train_df_all_drop_features_20: (500, 21)\n",
      "Feature size: 30 for train_df_all_drop\n",
      "Shape of train_df_all_drop_features_30: (500, 31)\n",
      "Feature size: 50 for train_df_all_drop\n",
      "Shape of train_df_all_drop_features_50: (500, 51)\n"
     ]
    }
   ],
   "source": [
    "# Define the different feature sizes to select\n",
    "feature_sizes = [5, 10, 15, 20, 30, 50]\n",
    "\n",
    "# Function to select top features, create new DataFrames, and save them\n",
    "def select_and_save_top_features(mutual_info_df, train_df, dataset_name, target_column='target'):\n",
    "    for size in feature_sizes:\n",
    "        # Step 1: Identify the top features with the highest mutual information\n",
    "        top_features = mutual_info_df.head(size)['Feature']\n",
    "\n",
    "        # Step 2: Create train_df_features_selected\n",
    "        train_df_features_selected = train_df.loc[:, top_features].copy()\n",
    "\n",
    "        # Add the target column back to the train_df_features_selected\n",
    "        train_df_features_selected.loc[:, target_column] = train_df[target_column].values\n",
    "\n",
    "        # Define the variable name\n",
    "        if size == len(mutual_info_df):\n",
    "            train_var_name = f'{dataset_name}_features_all'\n",
    "        else:\n",
    "            train_var_name = f'{dataset_name}_features_{size}'\n",
    "\n",
    "        # Dynamically create a variable to store the DataFrame\n",
    "        globals()[train_var_name] = train_df_features_selected\n",
    "\n",
    "        # Save the DataFrame as a .pkl file\n",
    "        train_df_features_selected.to_pickle(f\"feature_selection/{train_var_name}.pkl\")\n",
    "\n",
    "        # Display the shape of the resulting DataFrame to confirm\n",
    "        print(f\"Feature size: {size} for {dataset_name}\")\n",
    "        print(f\"Shape of {train_var_name}:\", train_df_features_selected.shape)\n",
    "\n",
    "# Apply the function for both datasets\n",
    "\n",
    "# For train_df_all (drop_first=False)\n",
    "print(\"Processing train_df_all (drop_first=False):\")\n",
    "select_and_save_top_features(mutual_info_all, train_df_all, 'train_df_all')\n",
    "\n",
    "# For train_df_all_drop (drop_first=True)\n",
    "print(\"\\nProcessing train_df_all_drop (drop_first=True):\")\n",
    "select_and_save_top_features(mutual_info_all_drop, train_df_all_drop, 'train_df_all_drop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### select different feature sizes from test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test_df_all (drop_first=False):\n",
      "Feature size: 5 for test_df_all\n",
      "Shape of test_df_all_features_5: (500, 10)\n",
      "Feature size: 10 for test_df_all\n",
      "Shape of test_df_all_features_10: (500, 15)\n",
      "Feature size: 15 for test_df_all\n",
      "Shape of test_df_all_features_15: (500, 20)\n",
      "Feature size: 20 for test_df_all\n",
      "Shape of test_df_all_features_20: (500, 25)\n",
      "Feature size: 30 for test_df_all\n",
      "Shape of test_df_all_features_30: (500, 35)\n",
      "Feature size: 50 for test_df_all\n",
      "Shape of test_df_all_features_50: (500, 55)\n",
      "\n",
      "Processing test_df_all_drop (drop_first=True):\n",
      "Feature size: 5 for test_df_all_drop\n",
      "Shape of test_df_all_drop_features_5: (500, 10)\n",
      "Feature size: 10 for test_df_all_drop\n",
      "Shape of test_df_all_drop_features_10: (500, 15)\n",
      "Feature size: 15 for test_df_all_drop\n",
      "Shape of test_df_all_drop_features_15: (500, 20)\n",
      "Feature size: 20 for test_df_all_drop\n",
      "Shape of test_df_all_drop_features_20: (500, 25)\n",
      "Feature size: 30 for test_df_all_drop\n",
      "Shape of test_df_all_drop_features_30: (500, 35)\n",
      "Feature size: 50 for test_df_all_drop\n",
      "Shape of test_df_all_drop_features_50: (500, 55)\n"
     ]
    }
   ],
   "source": [
    "# Function to select top features from test datasets based on selected train features\n",
    "def select_test_features(mutual_info_df, train_dataset_name, test_df, test_dataset_name, target_column='target'):\n",
    "    for size in feature_sizes:\n",
    "        # Step 1: Define the train file name and load the corresponding train DataFrame from the .pkl file\n",
    "        if size == len(mutual_info_df):\n",
    "            train_var_name = f'{train_dataset_name}_features_all'\n",
    "        else:\n",
    "            train_var_name = f'{train_dataset_name}_features_{size}'\n",
    "\n",
    "        # Load the train DataFrame from the corresponding .pkl file\n",
    "        train_df_features_selected = pd.read_pickle(f\"feature_selection/{train_var_name}.pkl\")\n",
    "\n",
    "        # Ensure the train DataFrame was loaded correctly\n",
    "        if train_df_features_selected is None:\n",
    "            print(f\"Train DataFrame for feature size {size} not found!\")\n",
    "            continue\n",
    "\n",
    "        # Step 2: Identify the selected features including 'target'\n",
    "        train_top_features = train_df_features_selected.columns  # Includes 'target'\n",
    "\n",
    "        # Step 3: Select the same features from the test dataset, including 'target' if available in test\n",
    "        test_top_features = train_top_features.intersection(test_df.columns)\n",
    "        test_df_features_selected = test_df.loc[:, test_top_features]\n",
    "\n",
    "        # Step 4: Retain the special columns and move them to the last 4 positions\n",
    "        special_columns_in_test = [col for col in special_columns if col in test_df.columns]\n",
    "        test_df_features_selected = pd.concat([test_df_features_selected, test_df[special_columns_in_test]], axis=1)\n",
    "\n",
    "        # Define the test variable name\n",
    "        if size == len(mutual_info_df):\n",
    "            test_var_name = f'{test_dataset_name}_features_all'\n",
    "        else:\n",
    "            test_var_name = f'{test_dataset_name}_features_{size}'\n",
    "\n",
    "        # Save the test DataFrame as a .pkl file\n",
    "        test_df_features_selected.to_pickle(f\"feature_selection/{test_var_name}.pkl\")\n",
    "\n",
    "        # Display the shape of the resulting test DataFrame to confirm\n",
    "        print(f\"Feature size: {size} for {test_dataset_name}\")\n",
    "        print(f\"Shape of {test_var_name}:\", test_df_features_selected.shape)\n",
    "\n",
    "# Apply the function for both test datasets\n",
    "\n",
    "# For test_df_all (drop_first=False)\n",
    "print(\"Processing test_df_all (drop_first=False):\")\n",
    "select_test_features(mutual_info_all, 'train_df_all', test_df_all, 'test_df_all')\n",
    "\n",
    "# For test_df_all_drop (drop_first=True)\n",
    "print(\"\\nProcessing test_df_all_drop (drop_first=True):\")\n",
    "select_test_features(mutual_info_all_drop, 'train_df_all_drop', test_df_all_drop, 'test_df_all_drop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated train_df_all_features_10_PRS shape (drop_first=False): (500, 47)\n",
      "Updated test_df_all_features_10_PRS shape (drop_first=False): (500, 55)\n",
      "Updated train_df_all_drop_features_10_PRS shape (drop_first=True): (500, 47)\n",
      "Updated test_df_all_drop_features_10_PRS shape (drop_first=True): (500, 55)\n"
     ]
    }
   ],
   "source": [
    "# Function to add PRS columns before the target column in the feature-selected DataFrames\n",
    "def add_columns_before_target(full_df, feature_df, columns_to_add, special_columns=None):\n",
    "    # Extract the columns to add from the full dataset\n",
    "    new_columns = full_df[columns_to_add]\n",
    "\n",
    "    # Drop the target column temporarily from the feature-selected DataFrame\n",
    "    target = feature_df['target']\n",
    "    feature_df = feature_df.drop(columns=['target'])\n",
    "\n",
    "    # Concatenate the new columns and the original feature-selected DataFrame\n",
    "    updated_df = pd.concat([new_columns, feature_df], axis=1)\n",
    "\n",
    "    # Add the target column back\n",
    "    updated_df['target'] = target\n",
    "\n",
    "    # For test datasets: Retain the special columns and move them after 'target'\n",
    "    if special_columns is not None:\n",
    "        special_columns_in_test = [col for col in special_columns if col in full_df.columns]\n",
    "        updated_df = pd.concat([updated_df, full_df[special_columns_in_test]], axis=1)\n",
    "\n",
    "    return updated_df\n",
    "\n",
    "# Load the train and test DataFrames from .pkl files\n",
    "train_df_all_features_10 = pd.read_pickle(\"feature_selection/train_df_all_features_10.pkl\")\n",
    "test_df_all_features_10 = pd.read_pickle(\"feature_selection/test_df_all_features_10.pkl\")\n",
    "train_df_all_drop_features_10 = pd.read_pickle(\"feature_selection/train_df_all_drop_features_10.pkl\")\n",
    "test_df_all_drop_features_10 = pd.read_pickle(\"feature_selection/test_df_all_drop_features_10.pkl\")\n",
    "\n",
    "# Apply the function to each dataset\n",
    "\n",
    "# For train_df_all_features_10 (drop_first=False)\n",
    "train_df_all_features_10_PRS = add_columns_before_target(train_df_all, train_df_all_features_10, PRS_columns)\n",
    "\n",
    "# For test_df_all_features_10 (drop_first=False), include special columns\n",
    "test_df_all_features_10_PRS = add_columns_before_target(test_df_all, test_df_all_features_10, PRS_columns, special_columns)\n",
    "\n",
    "# For train_df_all_drop_features_10 (drop_first=True)\n",
    "train_df_all_drop_features_10_PRS = add_columns_before_target(train_df_all_drop, train_df_all_drop_features_10, PRS_columns)\n",
    "\n",
    "# For test_df_all_drop_features_10 (drop_first=True), include special columns\n",
    "test_df_all_drop_features_10_PRS = add_columns_before_target(test_df_all_drop, test_df_all_drop_features_10, PRS_columns, special_columns)\n",
    "\n",
    "# Print updated DataFrame shapes to confirm\n",
    "print(\"Updated train_df_all_features_10_PRS shape (drop_first=False):\", train_df_all_features_10_PRS.shape)\n",
    "print(\"Updated test_df_all_features_10_PRS shape (drop_first=False):\", test_df_all_features_10_PRS.shape)\n",
    "print(\"Updated train_df_all_drop_features_10_PRS shape (drop_first=True):\", train_df_all_drop_features_10_PRS.shape)\n",
    "print(\"Updated test_df_all_drop_features_10_PRS shape (drop_first=True):\", test_df_all_drop_features_10_PRS.shape)\n",
    "\n",
    "# Save the DataFrames with the suffix '_PRS'\n",
    "train_df_all_features_10_PRS.to_pickle(\"feature_selection/train_df_all_features_10_PRS.pkl\")\n",
    "test_df_all_features_10_PRS.to_pickle(\"feature_selection/test_df_all_features_10_PRS.pkl\")\n",
    "train_df_all_drop_features_10_PRS.to_pickle(\"feature_selection/train_df_all_drop_features_10_PRS.pkl\")\n",
    "test_df_all_drop_features_10_PRS.to_pickle(\"feature_selection/test_df_all_drop_features_10_PRS.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logistic regression with Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features for train_df_all (drop_first=False): Index(['p34', 'p3086_i0', 'p4103_i0', 'p4120_i0', 'p23032_i0', 'p23033_i0',\n",
      "       'p23037_i0', 'p26203', 'p26269', 'p30230_i0'],\n",
      "      dtype='object')\n",
      "Selected features for train_df_all_drop (drop_first=True): Index(['p34', 'p3086_i0', 'p4103_i0', 'p4120_i0', 'p23032_i0', 'p23033_i0',\n",
      "       'p23037_i0', 'p26203', 'p26269', 'p30230_i0'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Function to perform logistic regression with Lasso and select 10 features\n",
    "def select_top_features_via_logistic_lasso(train_df, target_column='target', num_features=10, threshold=0.0001):\n",
    "    # Separate features and target\n",
    "    X = train_df.drop(columns=[target_column])\n",
    "    y = train_df[target_column]\n",
    "    \n",
    "    # Initialize Logistic Regression with L1 regularization (Lasso)\n",
    "    model = LogisticRegression(penalty='l1', solver='liblinear', max_iter=10000, C=1.0)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Get the coefficients\n",
    "    coefficients = np.abs(model.coef_)[0]\n",
    "    \n",
    "    # Keep adjusting regularization until only num_features have coefficients > threshold\n",
    "    while np.sum(coefficients > threshold) > num_features:\n",
    "        model = LogisticRegression(penalty='l1', solver='liblinear', max_iter=10000, C=model.C * 0.99) \n",
    "        model.fit(X, y)\n",
    "        coefficients = np.abs(model.coef_)[0]\n",
    "    \n",
    "    # Extract the selected features (those with non-zero coefficients > threshold)\n",
    "    selected_features = X.columns[coefficients > threshold]\n",
    "    \n",
    "    return selected_features\n",
    "\n",
    "# Apply the function to both datasets (drop_first=False and drop_first=True)\n",
    "selected_features_all = select_top_features_via_logistic_lasso(train_df_all, target_column='target', num_features=10)\n",
    "selected_features_all_drop = select_top_features_via_logistic_lasso(train_df_all_drop, target_column='target', num_features=10)\n",
    "\n",
    "# Print the selected features\n",
    "print(\"Selected features for train_df_all (drop_first=False):\", selected_features_all)\n",
    "print(\"Selected features for train_df_all_drop (drop_first=True):\", selected_features_all_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def select_features_from_datasets(train_df, test_df, selected_features, target_column='target', special_columns=None):\n",
    "    # Select features + target column for train dataset\n",
    "    train_df_selected = train_df[selected_features.tolist() + [target_column]].copy()\n",
    "    \n",
    "    # Select features + target column for test dataset\n",
    "    test_df_selected = test_df[selected_features.tolist() + [target_column]].copy()\n",
    "    \n",
    "    # If special columns exist, add them to the test dataset\n",
    "    if special_columns is not None:\n",
    "        special_columns_in_test = [col for col in special_columns if col in test_df.columns]\n",
    "        test_df_selected = pd.concat([test_df_selected, test_df[special_columns_in_test]], axis=1)\n",
    "    \n",
    "    return train_df_selected, test_df_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df_all_features_10_logistic shape (drop_first=False): (500, 11)\n",
      "test_df_all_features_10_logistic shape (drop_first=False): (500, 15)\n",
      "train_df_all_drop_features_10_logistic shape (drop_first=True): (500, 11)\n",
      "test_df_all_drop_features_10_logistic shape (drop_first=True): (500, 15)\n"
     ]
    }
   ],
   "source": [
    "# Apply the function to create the new datasets\n",
    "\n",
    "# For the all dataset (drop_first=False, with _features_10_logistic suffix)\n",
    "train_df_all_features_10_logistic, test_df_all_features_10_logistic = select_features_from_datasets(\n",
    "    train_df_all, test_df_all, selected_features_all, target_column='target', special_columns=special_columns)\n",
    "\n",
    "# For the all_drop dataset (drop_first=True, with _features_10_logistic suffix)\n",
    "train_df_all_drop_features_10_logistic, test_df_all_drop_features_10_logistic = select_features_from_datasets(\n",
    "    train_df_all_drop, test_df_all_drop, selected_features_all_drop, target_column='target', special_columns=special_columns)\n",
    "\n",
    "# Print shapes to confirm the new datasets\n",
    "print(\"train_df_all_features_10_logistic shape (drop_first=False):\", train_df_all_features_10_logistic.shape)\n",
    "print(\"test_df_all_features_10_logistic shape (drop_first=False):\", test_df_all_features_10_logistic.shape)\n",
    "print(\"train_df_all_drop_features_10_logistic shape (drop_first=True):\", train_df_all_drop_features_10_logistic.shape)\n",
    "print(\"test_df_all_drop_features_10_logistic shape (drop_first=True):\", test_df_all_drop_features_10_logistic.shape)\n",
    "\n",
    "# Save the new datasets with the suffix '_features_10_logistic'\n",
    "train_df_all_features_10_logistic.to_pickle(\"feature_selection/train_df_all_features_10_logistic.pkl\")\n",
    "test_df_all_features_10_logistic.to_pickle(\"feature_selection/test_df_all_features_10_logistic.pkl\")\n",
    "train_df_all_drop_features_10_logistic.to_pickle(\"feature_selection/train_df_all_drop_features_10_logistic.pkl\")\n",
    "test_df_all_drop_features_10_logistic.to_pickle(\"feature_selection/test_df_all_drop_features_10_logistic.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## manual feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "manual_selected_features = pd.Index([\n",
    "    'p47_i0',\n",
    "    'p135_i0',\n",
    "    'p137_i0',\n",
    "    'p22189',\n",
    "    'p30140_i0',\n",
    "    'p46_i0',\n",
    "    'p47_i0',\n",
    "    'p48_i0',\n",
    "    'p49_i0',\n",
    "    'p50_i0',\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df_all_features_10_manual shape (drop_first=False): (500, 11)\n",
      "test_df_all_features_10_manual shape (drop_first=False): (500, 15)\n",
      "train_df_all_drop_features_10_manual shape (drop_first=True): (500, 11)\n",
      "test_df_all_drop_features_10_manual shape (drop_first=True): (500, 15)\n"
     ]
    }
   ],
   "source": [
    "# For the all dataset (drop_first=False, with _features_10_manual suffix)\n",
    "train_df_all_features_10_manual, test_df_all_features_10_manual = select_features_from_datasets(\n",
    "    train_df_all, test_df_all, manual_selected_features, target_column='target', special_columns=special_columns)\n",
    "\n",
    "# For the all_drop dataset (drop_first=True, with _features_10_manual suffix)\n",
    "train_df_all_drop_features_10_manual, test_df_all_drop_features_10_manual = select_features_from_datasets(\n",
    "    train_df_all_drop, test_df_all_drop, manual_selected_features, target_column='target', special_columns=special_columns)\n",
    "\n",
    "# Print shapes to confirm the new datasets\n",
    "print(\"train_df_all_features_10_manual shape (drop_first=False):\", train_df_all_features_10_manual.shape)\n",
    "print(\"test_df_all_features_10_manual shape (drop_first=False):\", test_df_all_features_10_manual.shape)\n",
    "print(\"train_df_all_drop_features_10_manual shape (drop_first=True):\", train_df_all_drop_features_10_manual.shape)\n",
    "print(\"test_df_all_drop_features_10_manual shape (drop_first=True):\", test_df_all_drop_features_10_manual.shape)\n",
    "\n",
    "# Save the new datasets with the suffix '_features_10_manual'\n",
    "train_df_all_features_10_manual.to_pickle(\"feature_selection/train_df_all_features_10_manual.pkl\")\n",
    "test_df_all_features_10_manual.to_pickle(\"feature_selection/test_df_all_features_10_manual.pkl\")\n",
    "train_df_all_drop_features_10_manual.to_pickle(\"feature_selection/train_df_all_drop_features_10_manual.pkl\")\n",
    "test_df_all_drop_features_10_manual.to_pickle(\"feature_selection/test_df_all_drop_features_10_manual.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FRP31110",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
